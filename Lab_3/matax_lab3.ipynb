{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/master/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RgtXlfYO_i7"
   },
   "source": [
    "### **Lab 3: Iterative methods**\n",
    "**Mathias Axelsson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x_J5FVuPzbm"
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJipbXtnjrJZ"
   },
   "source": [
    "This report will implement functions for Jacobi and Gauss-Seidel iterations for $Ax =b$ as well as two functions for newton iterations. One for the case of a scalar function and another for the case of a vector equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkT8J7uOWpT3"
   },
   "source": [
    "#**About the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmB2noTr1Oyo"
   },
   "source": [
    "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pdll1Xc9WP0e",
    "outputId": "6435158d-714b-4fa0-f688-bbd72fa2bc02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
    "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
    "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
    "\n",
    "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This template is maintained by Johan Hoffman\n",
    "# Please report problems to jhoffman@kth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28xLGz8JX3Hh"
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2PYNusD08Wa"
   },
   "source": [
    "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw7VlErAX7NS"
   },
   "outputs": [],
   "source": [
    "# Load neccessary modules.\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import tri\n",
    "from matplotlib import axes\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnO3lhAigLev"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5zMzgPlRAF6"
   },
   "source": [
    "In this report functions for Jacobi and Gauss-Seidel iterations for $Ax =b$ as well as two functions for newton iterations will be implemented with one for scalar functions and one for vector functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOQvukXZq5U5"
   },
   "source": [
    "# **Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Jacobi iterations for $Ax =b$\n",
    "The Jacobi iteration uses matrix splitting to precondition the system. It uses the matrix split $A = A_1 + A_2$ where $A_1$ is chosen as diagonal. This results in the iteration matrix $M_J = D^{-1}(A-D)$ (Example 7.8 Chapter 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_solver(A, b):\n",
    "    D = np.diag(np.diag(A))\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    Dinv = np.linalg.inv(D)\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(np.eye(n) - np.matmul(Dinv, A)) >= 1:\n",
    "        print(\"Convergence criterion not met.\")\n",
    "        print(np.linalg.norm(np.eye(n) - np.matmul(Dinv, A)))\n",
    "        raise exception\n",
    "    \n",
    "    x = np.zeros(n)\n",
    "    r = b - np.dot(A, x)\n",
    "    \n",
    "    # Construct iteration matrix\n",
    "    Mj = -np.matmul(Dinv, (A - D))\n",
    "    \n",
    "    c = np.matmul(Dinv, b)\n",
    "    \n",
    "    \n",
    "    while np.linalg.norm(r) > 1e-10:\n",
    "        x = c + np.matmul(Mj, x)\n",
    "        r = b - np.dot(A, x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Gauss-Seidel iterations for $Ax =b$\n",
    "The Gauss-Seidel iteration uses a similar method to the Jacobi iteration. It instead uses the matrix $A = A_1 + A_2$ where $A_1$ is chosen as lower triangular. This results in the iteration matrix $M_J = L^{-1}(A-L)$ (Example 7.9 Chapter 7). Using forward substitution the matrix L is inverted resulting in the iteration\n",
    "$$\n",
    "    x^{(k+1)}_i = a_{ii}^{-1}\\left(b_i - \\sum_{j<i}a_{ij}x^{(k+1)}_j - \\sum_{j>i}a_{ij}x^{(k)}_j \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel_solver(A, b):\n",
    "    # Get lower triangular of A\n",
    "    L = np.tril(A)\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    Linv = np.linalg.inv(L)\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(np.eye(n) - np.matmul(Linv, A)) >= 1:\n",
    "        print(\"Convergence criterion not met.\")\n",
    "        print(np.linalg.norm(np.eye(n) - np.matmul(Linv, A)))\n",
    "        raise exception\n",
    "    \n",
    "    x = np.zeros(n)\n",
    "    r = b - np.dot(A, x)\n",
    "    \n",
    "    while np.linalg.norm(r) > 1e-10:\n",
    "        for i in range(n):\n",
    "            s = b[i]\n",
    "            for j in range(i):\n",
    "                s -= A[i,j]*x[j]\n",
    "            for j in range(i+1, n):\n",
    "                s -= A[i,j]*x[j]\n",
    "            x[i] = s/A[i,i]\n",
    "        r = b - np.dot(A, x)\n",
    "    return x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Newton iteration (scalar)\n",
    "The Newton iteartion is a fixed point iteration with $\\alpha = f'(x)^{-1}$ (Example 8.9 Chapter 8)\n",
    "$$\n",
    "    x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_newton(f, x0=0, e=1e-5):\n",
    "    x = x0\n",
    "    while np.abs(f(x)) > 1e-10:\n",
    "        df = (f(x+e) - f(x))/e\n",
    "        if np.linalg.norm(df) < 1e-15:\n",
    "            print(\"Aborting, df too large\")\n",
    "            print(\"Returning current solution.\")\n",
    "            return x\n",
    "        x = x - f(x)/df\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Newton iterations (vector)\n",
    "The vector newton iterations start with the same premise as the scalar newton iterations. Then since \n",
    "$$\n",
    "    x^{(k+1)} = x^{(k)} - f'(x^{(k)})^{-1}f(x^{(k)}) \\Longleftrightarrow  f'(x^{(k)})(x^{(k+1)} - x^{(k)}) = -f(x^{(k)})\n",
    "$$\n",
    "the difference between each iteration can be solved for using a linear equation system solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_newton(f, x0, e=1e-5):\n",
    "    # Init Jacobian matrix\n",
    "    m = f(x0).shape[0]\n",
    "    n = x0.shape[0]\n",
    "    \n",
    "    Df = np.zeros((m, n))\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    while np.linalg.norm(f(x)) > 1e-10:\n",
    "        # Calculate Jacobian matrix at x\n",
    "        f0 = f(x)\n",
    "        for j in range(n):\n",
    "            eps = np.zeros(x.size)\n",
    "            eps[j] = e\n",
    "            Df[:, j] = (f(x + eps) - f0)/e\n",
    "        dx = np.linalg.solve(Df, -f(x))\n",
    "        if np.linalg.norm(dx) < 1e-15:\n",
    "            print(\"Aborting, no steps are made.\")\n",
    "            print(\"Returning current solution.\")\n",
    "            return x\n",
    "        x = x + dx\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsQLT38gVbn_"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLwlnOzuV-Cd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ax = b solvers\n",
    "for i in range(100):\n",
    "    A = np.random.randint(-100, high=100, size=(5,5))/20\n",
    "    \n",
    "    # Make A diagonally dominant for convergence\n",
    "    D = np.diag(np.random.randint(5, high=10, size=5))*5\n",
    "    \n",
    "    A = A + D\n",
    "    if np.linalg.norm(np.eye(5) - np.matmul(np.linalg.inv(D), A)) >= 1:\n",
    "        continue\n",
    "    \n",
    "    y = np.random.randint(100, size=5)\n",
    "    \n",
    "    b = np.matmul(A,y)\n",
    "    \n",
    "    x1 = jacobi_solver(A, b)\n",
    "    x2 = gauss_seidel_solver(A, b)\n",
    "    \n",
    "    assert np.linalg.norm(np.matmul(A, x1) - b) < 1e-8\n",
    "    assert np.linalg.norm(np.matmul(A, x2) - b) < 1e-8\n",
    "    assert np.linalg.norm(x1 - y) < 1e-8\n",
    "    assert np.linalg.norm(x2 - y) < 1e-8\n",
    "\n",
    "# Scalar newton\n",
    "for i in range(10):\n",
    "    \n",
    "    # Size\n",
    "    m = 1\n",
    "    n = 4 # Number of coefficients should be even to ensure that a solution exists.\n",
    "    \n",
    "    # Coefficients\n",
    "    A = np.random.randint(-100, high=100, size=(m,n))/100\n",
    "\n",
    "    # I'm sorry for this one. \n",
    "    # It creates a matrix of 1, x, x^2 ... x^5 (np.array([x[i]**j for i in range(6) for j in range(6)]).reshape(6,6))\n",
    "    # Multiplies in coefficients so that the matrix is a_0, a_1x^1 ... a_5 x^5\n",
    "    # And then adds the rows together to a vector function.\n",
    "    f = lambda x: np.matmul(np.multiply(A,np.array([x[i]**j for i in range(m) for j in range(n)]).reshape(m,n)), np.ones(n))\n",
    "    \n",
    "    # Random initial guess\n",
    "    y = np.random.randint(-10, high=10, size=m)\n",
    "\n",
    "    x = scalar_newton(f, y)\n",
    "    \n",
    "    assert np.linalg.norm(f(x)) < 1e-8\n",
    "\n",
    "# Vector newton\n",
    "for i in range(10):\n",
    "    \n",
    "    # Size\n",
    "    m = 5\n",
    "    n = 4 # Number of coefficients should be even to ensure that a solution exists.\n",
    "    \n",
    "    # Coefficients\n",
    "    A = np.random.randint(-100, high=100, size=(m,n))/100\n",
    "\n",
    "    # I'm sorry for this one. \n",
    "    # It creates a matrix of 1, x, x^2 ... x^5 (np.array([x[i]**j for i in range(6) for j in range(6)]).reshape(6,6))\n",
    "    # Multiplies in coefficients so that the matrix is a_0, a_1x^1 ... a_5 x^5\n",
    "    # And then adds the rows together to a vector function.\n",
    "    f = lambda x: np.matmul(np.multiply(A,np.array([x[i]**j for i in range(m) for j in range(n)]).reshape(m,n)), np.ones(n))\n",
    "    \n",
    "    # Random initial guess\n",
    "    y = np.random.randint(-10, high=10, size=m)\n",
    "\n",
    "    x = vector_newton(f, y)\n",
    "    \n",
    "    assert np.linalg.norm(f(x)) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4GLBv0zWr7m"
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bcsDSoRXHZe"
   },
   "source": [
    "The functions work as expected. The newton solver for vector functions requires that the Jacobian is full rank to be able to iterate. This is an assumption for the method in the book and therefore I will leave it at that. I could have used the Jacobian or Gauss-Seidel solvers for this step. But since they do not converge for all matrices I have opted to use a general solver from numpy. Test cases for the newton methods are contructed with polynomials that have their largest exponent set as odd so that there always exists a solution. I had problems with finding a way to ensure that $A$ was diagonally dominant so that the Jacobi solver would converge. I therefore added an if-statement that skips any matrices that would not converge for the Jacobi solver.\n",
    "\n",
    "I have some convergence problems with both of the newton solvers. I do not believe that it is due to my implementation but rather because of the random test cases. Since the generation of random polynomials with mixed terms (Nondiagonal Jacobian) would further complicate the generation code I have chosen to omit those polynomials."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "template-report-lab-X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
