{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "szaboda_lab3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 3: Iterative methods**\n",
        "**Dániel Szabó**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "This laboratory is mostly about approximately solving a system of linear equations given in the form $Ax=b$ using iterative methods. The mandatory tasks are the implementation of the Jacobi method and the Gauss-Seidel method for solving system $Ax=b$, and Newton's method for finding a root of a nonlinear function $f: \\mathbb{R}\\to\\mathbb{R}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89bd013d-4413-4436-841d-ca80a14455a5"
      },
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2021 Dániel Szabó (dszabo@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "Some details about the solved problems are presented here.\n",
        "\n",
        "1. Jacobi iteration for $Ax=b$: The input is a real, quadratic matrix $A\\in\\mathbb{R}^{n×n}$ and a real vector $b\\in\\mathbb{R}^n$. The task is to approximate $x\\in\\mathbb{R}^n$ that satisfies $Ax=b$ by performing iterations $x^{(k+1)}=(I-D^{-1}A)x^{(k)}+D^{-1}b$ where $D\\in\\mathbb{R}^{n×n}$ is the diagonal version of $A$ (i.e. $\\forall i,j\\in[n]\\ d_{ij}=a_{ii}$ if $i=j$, otherwise $d_{ij}=0$).\n",
        "\n",
        "2. Gauss-Seidel iteration for $Ax=b$: The input is a real, quadratic matrix $A\\in\\mathbb{R}^{n×n}$ and a real vector $b\\in\\mathbb{R}^n$. The task is to approximate $x\\in\\mathbb{R}^n$ that satisfies $Ax=b$ by performing iterations $x^{(k+1)}=(I-L^{-1}A)x^{(k)}+L^{-1}b$ where $L\\in\\mathbb{R}^{n×n}$ is the lower triangular version of $A$ (i.e. $\\forall i,j\\in[n]\\ l_{ij}=0$ if $i<j$, otherwise $l_{ij}=a_{ij}$).\n",
        "\n",
        "3. Newton's method for scalar nonlinear equation $f(x)=0$: The input is a real function $f:\\mathbb{R}\\to\\mathbb{R}$. The task is to approximate a root of $f$, i.e. $x\\in\\mathbb{R}$ such that $f(x)=0$, by performing iterations $x^{(k+1)}=x^{(k)}-f(x^{(k)})/f'(x^{(k)})$ where function $f'$ is the derivative of $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWnmrVun5Es"
      },
      "source": [
        "Input validation: In most of the tasks we check if the input matrix is actually a matrix, i.e. if it has the same number of elements in each row, and it is done by the matrix_check function. Furthermore, in each task we also check if the input satisfies some other requirements, e.g. if the input matrix is a square matrix, if it is compatible with another input or if the precision parameter is positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnJJTezqn5Vy"
      },
      "source": [
        "def matrix_check(M):\r\n",
        "    for i in range(len(M)):\r\n",
        "        if(len(M[i])!=len(M[0])):\r\n",
        "            raise Exception(\"The number of elements in each row of a matrix should be the same.\")\r\n",
        "    return"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "1. Jacobi iteration for $Ax=b$: First, the validity of the input is checked. Then, the diagonal version $D$ of $A$ and $D$'s inverse is calculated. We check if the convergence criterion $||I-D^{-1}A||<1$ is satisfied: if it is not, we cannot continue so an exception is raised. Otherwise, we perform the iterations described in Example 7.8 of the lecture notes (that is, $x^{(k+1)}=(I-D^{-1}A)x^{(k)}+D^{-1}b$) until residual $||Ax-b||$ is smaller than the input precision parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEuran57sg1q"
      },
      "source": [
        "def jacobi_iteration(A, b, eps):\r\n",
        "    matrix_check(A)\r\n",
        "    n = len(A)\r\n",
        "    if n != len(A[0]):\r\n",
        "        raise Exception(\"The input matrix should be a square matrix.\")\r\n",
        "    if n != len(b):\r\n",
        "        raise Exception(\"The number of elements in the input vector should be the same as the size of the matrix.\")\r\n",
        "    if eps<=0:\r\n",
        "        raise Exception(\"Epsilon should be positive.\")\r\n",
        "    \r\n",
        "    D = np.diag(np.diag(A))\r\n",
        "    Dinv = np.linalg.inv(D)\r\n",
        "    if np.linalg.norm(np.subtract(np.eye(n), Dinv @ A)) >= 1:\r\n",
        "        raise Exception(\"convergence criterion not fulfilled\")\r\n",
        "    x = np.zeros(n)\r\n",
        "    residual = np.inf\r\n",
        "    while residual > eps:\r\n",
        "        x = np.add(np.dot(np.subtract(np.eye(n), Dinv @ A), x), Dinv @ b)\r\n",
        "        residual = np.linalg.norm(np.subtract(A @ x, b))\r\n",
        "    return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHvRxLba1ori"
      },
      "source": [
        "2. Gauss-Seidel iteration for $Ax=b$: First, the validity of the input is checked. Then, the lower triangular version $L$ of $A$ and $L$'s inverse is calculated. We check if the convergence criterion $||I-L^{-1}A||<1$ is satisfied: if it is not, we cannot continue so an exception is raised. Otherwise, we perform the iterations described in Example 7.9 of the lecture notes (that is, $x^{(k+1)}=(I-L^{-1}A)x^{(k)}+L^{-1}b$) until residual $||Ax-b||$ is smaller than the input precision parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoxAdKEltGFJ"
      },
      "source": [
        "def gauss_seidel_iteration(A, b, eps):\r\n",
        "    matrix_check(A)\r\n",
        "    n = len(A)\r\n",
        "    if n != len(A[0]):\r\n",
        "        raise Exception(\"The input matrix should be a square matrix.\")\r\n",
        "    if n != len(b):\r\n",
        "        raise Exception(\"The number of elements in the input vector should be the same as the size of the matrix.\")\r\n",
        "    if eps<=0:\r\n",
        "        raise Exception(\"Epsilon should be positive.\")\r\n",
        "    \r\n",
        "    L = np.tril(A)\r\n",
        "    Linv = np.linalg.inv(L)\r\n",
        "    if np.linalg.norm(np.subtract(np.eye(n), Linv @ A)) >= 1:\r\n",
        "        raise Exception(\"convergence criterion not fulfilled\")\r\n",
        "    x = np.zeros(n)\r\n",
        "    residual = np.inf\r\n",
        "    while residual > eps:\r\n",
        "        x = np.add(np.dot(np.subtract(np.eye(n), np.dot(Linv, A)), x), np.dot(Linv, b))\r\n",
        "        residual = np.linalg.norm(np.subtract(np.dot(A, x), b))\r\n",
        "    return x"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szfxL6C41rJS"
      },
      "source": [
        "3. Newton's method for scalar nonlinear equation $f(x)=0$: Let us suppose that $f$ is a polynomial, because this way, it is easy to calculate its derivative and also its value at a given point. This simplification can be justified by the fact that every infinitely differentiable function can be approximated by a polynomial, e.g. one of its Taylor polynomials. If $f$ is an $n^\\textrm{th}$-degree polynomial, then it can be given by an array containing its $n+1$ coefficients: if $f(x)=x_0+x_1x+\\dots+x_nx^n$, then the array is $[x_0,x_1,\\dots,x_n]$.\r\n",
        "\r\n",
        "  We have two auxiliary methods. The one named \"f\" calculates the value of a polynomial given by the array of its coefficients at a given point. The \"derivative\" calculates the derivative of a polynomial given by the array of its coefficients, so the result is another polynomial, also represented by its array of coefficients.\r\n",
        "\r\n",
        "  The main method (\"newton_scalar\") follows Algorithm 8.2 of the lecture notes. But instead of receiving an initial guess $x_0$ as an input, it generates some (at most ten) numbers randomly. For a certain $x_0$, we perform the iterations $x^{(k+1)}=x^{(k)}-f(x^{(k)})/f'(x^{(k)})$ until $f(x)$ is close enough to 0. We can also stop doing the iterations if we have already been doing it for a lot of time ($100+1/\\varepsilon$ steps), because the function may not have a real root at any point reachable by performing the iteration's steps from $x_0$. If we get a result for an $x_0$, we return it, otherwise an exception is raised, because in this case, with a high probability, the function does not have any real roots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hknFWIwZtR2K"
      },
      "source": [
        "def f(coeff, x):\r\n",
        "    result = 0\r\n",
        "    n = len(coeff)-1\r\n",
        "    for i in range(n+1):\r\n",
        "        result += coeff[i] * (x**i)\r\n",
        "    return result\r\n",
        "\r\n",
        "def derivative(coeff):\r\n",
        "    n = len(coeff)-1\r\n",
        "    der = []\r\n",
        "    for i in range(n):\r\n",
        "        der.append(coeff[i+1]*(i+1))\r\n",
        "    return der\r\n",
        "\r\n",
        "def newton_scalar(coeff, eps):\r\n",
        "    if eps<=0:\r\n",
        "        raise Exception(\"Epsilon should be positive.\")\r\n",
        "    for _ in range(10):\r\n",
        "        x0 = random.randint(-50, 50)\r\n",
        "        x = x0\r\n",
        "        fx = f(coeff, x)\r\n",
        "        limit = 100+1/eps\r\n",
        "        while np.abs(fx) > eps and limit > 0:\r\n",
        "            der = derivative(coeff)\r\n",
        "            df = f(der, x)\r\n",
        "            x -= fx/df\r\n",
        "            fx = f(coeff, x)\r\n",
        "            limit -= 1\r\n",
        "        if limit > 0:\r\n",
        "            return x\r\n",
        "    raise Exception(\"Couldn't find any root\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "The verification of the methods is done by computing the results for some random test data using the algorithms of the Method section and then calculating the error metrics that are required in the lab instructions using some numpy methods. Then we check if these values are close to 0. Some tests with invalid input are also provided.\r\n",
        "\r\n",
        "Since the inputs are generated randomly, it is possible that sometimes the Jacobi or the Gauss-Seidel method does not converge, or that Newton's method does not find a real root of the input function. In these cases, an exception is raised, and the corresponding test should be run again. Hopefully, there will be no need for too many experiments as the inputs are generated in a way so that the methods can work with them with high probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD0KtLcojziS"
      },
      "source": [
        "1. Jacobi iteration for $Ax=b$: We generate a random matrix $A$ that is diagonally dominant with high probability, because we know that $A$ being diagonally dominant is a sufficient condition for the method to be convergent. A random vector $b$ is generated as well, and the approximate solution is calculated with our method. Then we check if norms $||Ax-b||$ and $||x-y||$ are close to zero, where $y=A^{-1}b$. Invalid input handling is also tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wKucyOP0yOH",
        "outputId": "9dcc61d0-4f57-4274-b9e2-65a28855ae8b"
      },
      "source": [
        "n = random.randint(2,5)\r\n",
        "epsilon = 10**-5\r\n",
        "A = np.array([[random.gauss(0, 1) for _ in range(n)] for __ in range(n)])\r\n",
        "for i in range(n):\r\n",
        "    A[i,i] = random.gauss(0, 10)\r\n",
        "b = [random.gauss(0, 5) for _ in range(n)]\r\n",
        "x = jacobi_iteration(A,b, epsilon)\r\n",
        "print(\"A =\", A)\r\n",
        "print(\"b =\", b)\r\n",
        "print(\"x =\", x)\r\n",
        "\r\n",
        "norm1 = np.linalg.norm(np.subtract(np.dot(A,x), b))\r\n",
        "norm2 = np.linalg.norm(np.subtract(x, np.dot(np.linalg.inv(A), b)))\r\n",
        "print(\"errors:\", norm1, norm2)\r\n",
        "np.testing.assert_almost_equal(norm1, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "np.testing.assert_almost_equal(norm2, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "\r\n",
        "# Invalid input test: not compatible inputs\r\n",
        "np.testing.assert_raises(Exception, jacobi_iteration, np.array([[1,2],[3,4]]), np.array([1,2,3]))\r\n",
        "# Invalid input test: non-square matrix\r\n",
        "np.testing.assert_raises(Exception, jacobi_iteration, np.array([[1,2],[3,4],[5,6]]), np.array([1,2]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A = [[  1.61261773  -0.75843985   0.46207506]\n",
            " [  0.1934651  -10.52647335  -1.12791503]\n",
            " [ -0.49932479   0.47117408   8.72957704]]\n",
            "b = [0.830016173448243, -7.6694178766028145, -0.9214567588221791]\n",
            "x = [ 0.89712791  0.75525164 -0.09500515]\n",
            "errors: 4.965039609223026e-06 2.691781159671771e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q6R01BPj9xq"
      },
      "source": [
        "2. Gauss-Seidel iteration for $Ax=b$: We generate a random matrix $A$ that is diagonally dominant with high probability, because we know that $A$ being diagonally dominant is a sufficient condition for the method to be convergent. A random vector $b$ is generated as well, and the approximate solution is calculated with our method. Then we check if norms $||Ax-b||$ and $||x-y||$ are close to zero, where $y=A^{-1}b$. Invalid input handling is also tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXzWkz0H1a6f",
        "outputId": "cfbeda9b-3a05-461e-c049-bebbfa6eede1"
      },
      "source": [
        "n = random.randint(2,5)\r\n",
        "epsilon = 10**-5\r\n",
        "A = np.array([[random.gauss(0, 1) for _ in range(n)] for __ in range(n)])\r\n",
        "for i in range(n):\r\n",
        "    A[i,i] = random.gauss(0, 10)\r\n",
        "b = [random.gauss(0, 5) for _ in range(n)]\r\n",
        "x = gauss_seidel_iteration(A,b, epsilon)\r\n",
        "print(\"A =\", A)\r\n",
        "print(\"b =\", b)\r\n",
        "print(\"x =\", x)\r\n",
        "\r\n",
        "norm1 = np.linalg.norm(np.subtract(np.dot(A,x), b))\r\n",
        "norm2 = np.linalg.norm(np.subtract(x, np.dot(np.linalg.inv(A), b)))\r\n",
        "print(\"errors:\", norm1, norm2)\r\n",
        "np.testing.assert_almost_equal(norm1, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "np.testing.assert_almost_equal(norm2, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "\r\n",
        "# Invalid input test: not compatible inputs\r\n",
        "np.testing.assert_raises(Exception, gauss_seidel_iteration, np.array([[1,2],[3,4]]), np.array([1,2,3]))\r\n",
        "# Invalid input test: non-square matrix\r\n",
        "np.testing.assert_raises(Exception, gauss_seidel_iteration, np.array([[1,2],[3,4],[5,6]]), np.array([1,2]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A = [[ 1.98791248  0.20296918]\n",
            " [-0.14470231  9.09031026]]\n",
            "b = [-1.827721331162584, 1.0908590668029121]\n",
            "x = [-0.93015805  0.10519586]\n",
            "errors: 5.649281353825586e-08 2.8375640892033392e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnnKLV2ekBqx"
      },
      "source": [
        "3. Newton's method for scalar nonlinear equation $f(x)=0$: Function \"print_poly\" prints a polynomial given by its coefficients in a readable way. The degree of the input polynomial is a random integer between 2 and 7. The coefficients are also generated randomly. The exact roots (array $y$) are calculated with the numpy roots function, and one of the real roots ($x$) is computed with our function. Values $|f(x)|$ and $\\min_i|x-y_i|$ are calculated (the latter is the difference of the approximate root and the corresponding exact root), and we check if they are close to zero. Invalid input handling ($\\varepsilon\\le0$) is also tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eD4fJ-R1rUQ",
        "outputId": "6019f42c-7ec7-45a7-a926-2aa04e84e07b"
      },
      "source": [
        "def print_poly(coeffs):\r\n",
        "    print(coeffs[0], end='')\r\n",
        "    for i in range(1,len(coeffs)):\r\n",
        "        print(\" + \",coeffs[i],\" * x^\",i, end='', sep='')\r\n",
        "    print(\"\")\r\n",
        "\r\n",
        "n = random.randint(2,7)\r\n",
        "epsilon = 10**-5\r\n",
        "poly = [random.gauss(0, 1) for _ in range(n+1)]\r\n",
        "print_poly(poly)\r\n",
        "y = np.roots(poly[::-1])\r\n",
        "print(\"y =\",y)\r\n",
        "x = newton_scalar(poly, epsilon)\r\n",
        "print(\"x =\",x)\r\n",
        "\r\n",
        "err1 = np.abs(f(poly, x))\r\n",
        "err2 = np.min(abs(y-x))\r\n",
        "print(\"errors:\", err1, err2)\r\n",
        "np.testing.assert_almost_equal(err1, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "np.testing.assert_almost_equal(err2, 0, decimal=np.log10(1/epsilon)-1)\r\n",
        "\r\n",
        "# Invalid input test: non-positive epsilon\r\n",
        "np.testing.assert_raises(Exception, newton_scalar, [1,2,2,4], 0)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.024288678487018 + -1.673089745800391 * x^1 + -0.29340299027935385 * x^2\n",
            "y = [-6.26003573  0.55767481]\n",
            "x = 0.5576748060907004\n",
            "errors: 9.781203624825707e-12 4.8896442450541144e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The results were exactly as expected. All the methods implemented for solving the tasks, actually succeeded in solving them, as it is confirmed by the test results. Furthermore, the inputs are validated properly."
      ]
    }
  ]
}