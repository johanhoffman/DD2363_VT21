{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template-report-lab-X.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT21/blob/jacwah/Lab_3/jacwah_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 3: Iterative methods**\n",
        "**Jacob Wahlgren**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQ9rVmSTVQC"
      },
      "source": [
        "Iterative methods is a way to find approximate solutions to equations. Implementations of two algorithms for systems of linear equations (Jacobi, Gauss-Seidel), and one algorithm for scalar nonlinear equations (Newton) are presented. Randomized test cases verify the correctness of the implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "The code was written by the author (Jacob Wahlgren), based on a template by Johan Hoffman."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4cb0e8c5-a405-4ac4-c6f2-6ff68321c441"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "# Copyright (C) 2021 Jacob Wahlgren (jacobwah@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "from numpy.polynomial import Polynomial as P\n",
        "import unittest\n",
        "from functools import reduce"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "396j7uxpLWFA"
      },
      "source": [
        "Last week focused on direct methods of solving equations. This week the focus is iterative methods, i.e. where each step of the algorithm yields a better approximation of the exact solution. For linear equations, the benefit of iterative methods is that sparse matrices can be handled faster and with less memory than for direct methods. Iterative methods are also useful to solve nonlinear equations, since they cannot be solved with the direct methods shown previously.\n",
        "\n",
        "Jacobi iteration and Gauss-Seidel iteration are two similar methods of solving linear systems of equations $Ax=b$ based on stationary iteration. Statitionary iteration follows the pattern\n",
        "\n",
        "$\\displaystyle x^{(k+1)} = M x^{(k)} + c$\n",
        "\n",
        "for some constant matrix $M$ and vector $c$.\n",
        "\n",
        "Newton's method solves nonlinear equations by following the tangent line at each point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6u9MG5YjFZL"
      },
      "source": [
        "Both Jacobi iteration and Gauss-Seidel iteration are based on matrix splitting, where the matrix $A$ is split into $A=A_1+A_2$. The matrix $A_1$ is chosen to be easy to invert. They are defined in section 7.7 of the lecture notes. For Gauss-Seidel $A_1=\\text{diag}(A)$. Then use the following stationary system.\n",
        "\n",
        "$\\displaystyle M=I - A_1^{-1} A\\\\\n",
        "c = A_1^{-1} b$\n",
        "\n",
        "The iteration converges if $\\|M\\| < 1$. In the random test cases, this condition is tested, and if it does not hold a new instance is sampled. Sampling is slower the more dimensions are used, therefore the tests only go up to 4 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zen7JhW3rw67"
      },
      "source": [
        "def diag(A):\n",
        "  D = np.copy(A)\n",
        "  n = len(A)\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if i != j:\n",
        "        D[i,j] = 0\n",
        "  return D\n",
        "\n",
        "def jacobi_iteration(A, b):\n",
        "  D = diag(A)\n",
        "  n = len(A)\n",
        "  Di = np.linalg.inv(D)\n",
        "  M = np.identity(n) - Di@A\n",
        "  if np.linalg.norm(M, 2) >= 1:\n",
        "    raise ValueError(\"Matrix A does not fulfill convergence criterion\")\n",
        "  c = Di@b\n",
        "  x = np.zeros(n)\n",
        "  while np.linalg.norm(A@x - b) > 0.00000001:\n",
        "    x = M@x + c\n",
        "  return x\n",
        "\n",
        "class JacobiTest(unittest.TestCase):\n",
        "  def test_random(self):\n",
        "    for n in range(1, 5):\n",
        "      valid = False\n",
        "      while not valid:\n",
        "        try:\n",
        "          A = np.random.rand(n,n)\n",
        "          y = np.random.rand(n)\n",
        "          b = A@y\n",
        "          x = jacobi_iteration(A, b)\n",
        "          valid = True\n",
        "        except ValueError:\n",
        "          pass\n",
        "      with self.subTest(n=n, A=A, y=y, b=b):\n",
        "        np.testing.assert_almost_equal(A@x, b)\n",
        "        np.testing.assert_almost_equal(x, y)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M1Aa7Hmn41W"
      },
      "source": [
        "Since the only difference to Jacobi iteration is how to compute $A_1$, the implementation of Gauss-Seidel iteration is largely the same. For Gauss-Seidel $A_1$ is a lower triangular matrix, where the non-zero elements are equal to those of $A$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BNhq91Hko7-"
      },
      "source": [
        "def lower_triangular(A):\n",
        "  L = np.copy(A)\n",
        "  n = len(A)\n",
        "  for i in range(n):\n",
        "    for j in range(i):\n",
        "      L[i,j] = 0\n",
        "  return L\n",
        "\n",
        "def gauss_seidel_iteration(A, b):\n",
        "  L = lower_triangular(A)\n",
        "  Li = np.linalg.inv(L)\n",
        "  n = len(A)\n",
        "  M = np.identity(n) - Li@A\n",
        "  if np.linalg.norm(M, 2) >= 1:\n",
        "    raise ValueError(\"Matrix A does not fulfill convergence criterion\")\n",
        "  c = Li@b\n",
        "  x = np.zeros(n)\n",
        "  while np.linalg.norm(A@x - b) > 0.00000001:\n",
        "    x = M@x + c\n",
        "  return x\n",
        "\n",
        "class GaussSeidelTest(unittest.TestCase):\n",
        "  def test_random(self):\n",
        "    for n in range(1, 5):\n",
        "      valid = False\n",
        "      while not valid:\n",
        "        try:\n",
        "          A = np.random.rand(n,n)\n",
        "          y = np.random.rand(n)\n",
        "          b = A@y\n",
        "          x = gauss_seidel_iteration(A, b)\n",
        "          valid = True\n",
        "        except ValueError:\n",
        "          pass\n",
        "      with self.subTest(n=n, A=A, y=y, b=b):\n",
        "        np.testing.assert_almost_equal(A@x, b)\n",
        "        np.testing.assert_almost_equal(x, y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "270HxJDK9sUI"
      },
      "source": [
        "The formula for Newton's method is\n",
        "\n",
        "$\\displaystyle x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}$\n",
        "\n",
        "and is defined in section 8.3 of the lecture notes. The iteration is intialized by a \"guess\" $x_0$. The closer to the solution $x_0$ is, the faster the algorithm converges. I chose to use $x_0=0$ since the assignment specified that there should be no extra parameter.\n",
        "\n",
        "The implementation of Newton's method is based on the pseudocode in Algorithm 8.2 in the lecture notes. The function $f$ is here represented using Numpy's polynomial subpackage. There are two types of tests, one that samples random polynomial coefficients, and one that samples random polynomial roots. Since we are only looking for real solutions, polynomials with only complex roots are not used as test cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTfMwr7b7ksI"
      },
      "source": [
        "def newtons_method(f):\n",
        "  x = 0\n",
        "  df = f.deriv(1)\n",
        "  while abs(f(x)) > 0.000000001:\n",
        "    x = x - f(x)/df(x)\n",
        "  return x\n",
        "\n",
        "class NewtonTest(unittest.TestCase):\n",
        "  def test_random_coef(self):\n",
        "    for n in range(2, 10):\n",
        "      real = False\n",
        "      while not real:\n",
        "        y = np.random.rand(n)\n",
        "        f = P(y)\n",
        "        for root in f.roots():\n",
        "          if root.imag == 0:\n",
        "            real = True\n",
        "      with self.subTest(f=f):\n",
        "        x = newtons_method(f)\n",
        "        np.testing.assert_almost_equal(f(x), 0)\n",
        "\n",
        "  def test_random_solution(self):\n",
        "    for n in range(1, 10):\n",
        "      y = np.random.rand(n)\n",
        "      # (y1-x)(y2-x)...\n",
        "      f = reduce(lambda f, yi: f * P([-yi,1]), y, P([1]))\n",
        "      x = newtons_method(f)\n",
        "      with self.subTest(y=y, x=x, f=f):\n",
        "        np.testing.assert_almost_equal(f(x), 0)\n",
        "        self.assertTrue(np.isclose(x-y, 0, atol=1e-3).any())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "All the above defined test cases pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ_ySZWsfyN9",
        "outputId": "fc47cc35-314c-4e5d-a390-bee790a9ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "unittest.main(argv=[''], verbosity=1, exit=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.820s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fb69da3b198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "All the implemented methods are valid ways of solving different types of equations. Naively sampling problem instances works as a way of testing the solver, but as the dimensionality grows the likelihood of sampling a solvable problem instance can decrease. If possible, it may be faster to sample a random solution, and derive the problem instance instead."
      ]
    }
  ]
}