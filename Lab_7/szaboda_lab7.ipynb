{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "szaboda_lab7",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 7: Optimization and learning**\n",
        "**D치niel Szab칩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "This laboratory is about approximating the solutions of minimization problems. In a minimization problem, we are given a function $f: D\\to\\mathbb{R}$, where $D\\subseteq\\mathbb{R}^n$ is the search space. The goal is to find a point $x^*\\in D$ that satisfies $f(x^*)\\le f(x)$ for all $x\\in D$. In this report, two methods are presented for approximating such a solution: the fist one is gradient descent, and the other one is called Newton's method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pdll1Xc9WP0e",
        "outputId": "3e6031a7-1249-4bab-b044-2a5a38669958"
      },
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2021 D치niel Szab칩 (dszabo@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(0)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgAmbEtuk2Po"
      },
      "source": [
        "The first task is to implement the gradient descent method, and the second one is to implement Newton's method.\r\n",
        "\r\n",
        "The gradient descent (Algorithm 15.1. in the lecture notes) is an iterative method for finding a local minimum of an input funcion $f$. In each iteration, its next guess is in the opposite direction of the gradient of $f$ evaluated at the current point.\r\n",
        "$$x^{(k+1)}=x^{(k)}-\\alpha\\nabla f(x^{(k)})$$\r\n",
        "where $\\alpha$ is the step size that may depend on $f, \\nabla f(x^{(k)})$ and $x^{(k)}$. Note that an initial guess $x^{(0)}$ is needed. The iterations stop when the norm of the gradient is small enough.\r\n",
        "\r\n",
        "Newton's method (Algorithm 15.3. in the lecture notes) is also an iterative method, but it uses not only the first order derivatives (i.e. the gradient) of $f$, but also the second order derivatives that are the elements of the Hessian matrix $Hf$. The iterations are as follows.\r\n",
        "$$x^{(k+1)}=x^{(k)}-(Hf(x^{(k)}))^{-1}\\nabla f(x^{(k)})$$\r\n",
        "The iterations stop when the norm of the gradient is small enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M86Cgc1Gt1Lu"
      },
      "source": [
        "Method \"grad\" calculates the approximate gradient $\\nabla f$ of the given function $f:\\mathbb{R}^n\\to \\mathbb{R}$ at the given point $x\\in\\mathbb{R}^n$. It does so using the definition of the derivative: for a small positive real number $h$, the $i$'th element of the gradient (for all $i\\in[n]$) is approximated by $\\frac{f(x+he_i)-f(x)}{h}$, where $e_i$ is the $i$'th standard basis vector of $\\mathbb{R}^n$.\r\n",
        "\r\n",
        "Method \"hessian\" calculates the approximate Hessian of the given function $f:\\mathbb{R}^n\\to \\mathbb{R}$ at the given point $x\\in\\mathbb{R}^n$. It does so using the definition of the derivative: for a small positive real number $h$, the $i$'th row of the Hessian (for all $i\\in[n]$) is approximated by vector $\\frac{\\nabla f(x+he_i)-\\nabla f(x)}{h}$, where $e_i$ is the $i$'th standard basis vector of $\\mathbb{R}^n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i9IXt0a8IYM"
      },
      "source": [
        "def grad(f, x, h = 10**-8):\r\n",
        "    if not np.isscalar(f(x)):\r\n",
        "        raise Exception(\"The function should return a scalar value.\")\r\n",
        "\r\n",
        "    n = np.shape(x)[0]\r\n",
        "    ret = np.zeros(n)\r\n",
        "    for i in range(n):\r\n",
        "        x1 = list(x)\r\n",
        "        x1[i] += h\r\n",
        "        ret[i] = (f(x1)-f(x))/h\r\n",
        "    return ret\r\n",
        "\r\n",
        "def hessian(f, x, h = 10**-4):\r\n",
        "    if not np.isscalar(f(x)):\r\n",
        "        raise Exception(\"The function should return a scalar value.\")\r\n",
        "\r\n",
        "    n = np.shape(x)[0]\r\n",
        "    ret = np.zeros((n,n))\r\n",
        "    for i in range(n):\r\n",
        "        x1 = list(x)\r\n",
        "        x1[i] += h\r\n",
        "        ret[i,:] = (grad(f,x1)-grad(f,x))/h\r\n",
        "    return ret"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH_Suf2n_Uxb"
      },
      "source": [
        "Method \"get_alpha\" calculates the step length for the gradient descent method. It is a simple algorithm that receives $f$, $\\nabla f(x)$ and $x$ as input; checks some (by default 100) potential $\\alpha$ values in interval $[0,1]$, and returns the best, i.e. the one minimizing $f(x-\\alpha\\nabla f(x))$.\r\n",
        "\r\n",
        "Method \"gradient_descent\" works as described in the Introduction section and in Algoritm 15.1. of the lecture notes. The gradient is calculated by our method \"grad\". In some cases, because of finite precision, we may reach a state where $\\alpha$ is said to be 0 but the gradient is not small enough to stop the iterations. In this case, the algorithm would end up in an infinite loop. In order to avoid this, in each iteration it is checked if $\\alpha$ is zero, and if it is, the current result is returned together with printing a message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nf0yliaO1zd"
      },
      "source": [
        "def get_alpha(f, gradf, x, prec = 10**-2):\r\n",
        "    if not np.isscalar(f(x)):\r\n",
        "        raise Exception(\"The function should return a scalar value.\")\r\n",
        "    if np.shape(x)!=np.shape(gradf):\r\n",
        "        raise Exception(\"The gradient and x should have the same shape.\")\r\n",
        "\r\n",
        "    values = np.arange(0, 1, prec)\r\n",
        "    min_alpha = 0\r\n",
        "    for alpha in values:\r\n",
        "        fx = f(x-alpha*gradf)\r\n",
        "        if(fx < f(x-min_alpha*gradf)):\r\n",
        "            min_alpha = alpha\r\n",
        "    return min_alpha\r\n",
        "\r\n",
        "def gradient_descent(f, x0, eps = 10**-7):\r\n",
        "    if not np.isscalar(f(x0)):\r\n",
        "        raise Exception(\"The function should return a scalar value.\")\r\n",
        "\r\n",
        "    x = x0\r\n",
        "    gradf = grad(f,x)\r\n",
        "    while np.linalg.norm(gradf) > eps:\r\n",
        "        gradf = grad(f,x)\r\n",
        "        alpha = get_alpha(f, gradf, x)\r\n",
        "        if alpha==0:\r\n",
        "            print(\"The gradient is not small enough, but alpha is 0\")\r\n",
        "            return x\r\n",
        "        x = x - alpha*gradf\r\n",
        "    return x"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEYWPp8UDwVD"
      },
      "source": [
        "Method \"newton\" works as described in the Introduction section and in Algoritm 15.3. of the lecture notes. The gradient is calculated by our method \"grad\" and the Hessian by method \"hessian\". In some cases, because of finite precision, we may reach a state where the Hessian is said to be singular (thus, its inverse cannot be calculated) but the gradient is not small enough to stop the iterations. In this case, the algorithm would end up in an infinite loop. In order to avoid this, in each iteration it is checked if $Hf(x^{(k)})$ is full-rank, and if it is not, the current result is returned together with printing a message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcXVtDQZnRY0"
      },
      "source": [
        "def newton(f, x0, eps = 10**-5):\r\n",
        "    if not np.isscalar(f(x0)):\r\n",
        "        raise Exception(\"The function should return a scalar value.\")\r\n",
        "\r\n",
        "    x = x0\r\n",
        "    gradf = grad(f,x)\r\n",
        "    while np.linalg.norm(gradf) > eps:\r\n",
        "        gradf = grad(f,x)\r\n",
        "        hessf = hessian(f,x)\r\n",
        "        if np.linalg.matrix_rank(hessf)<len(x0):\r\n",
        "            print(\"The gradient is not small enough, but the Hessian is singular\")\r\n",
        "            return x\r\n",
        "        dx = np.linalg.solve(hessf,-gradf)\r\n",
        "        x += dx\r\n",
        "    return x"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGfU9jrlwFx8"
      },
      "source": [
        "For the verification of the tasks we need a function $f:\\mathbb{R}^n\\to\\mathbb{R}$. Method \"f\" can implement an arbitrary function, now $f(x)=\\sin(x_1)/x_1 + (x_2-c)^2$, where $c$ is a random number drawn from the standard normal distribution. The exact minimum point $x^*$ satisfies $\\tan(x_1)=x_1$ so $x_1\\approx\\pm4.4934094579090641753$; and $x_2=c$.\r\n",
        "\r\n",
        "The other example, \"g\" is from Exercise 15.7 of the lecture notes: $g(x)=2+x_1^4+(1+x_2)^2$. The exact solution is $x_1=0$, $x_2=-1$.\r\n",
        "\r\n",
        "A faulty function \"f_fail\" is also provided for testing the input validation: it returns a vector, not a scalar value.\r\n",
        "\r\n",
        "The results show that generally the approximations are close to the exact solutions. There is a larger error in the first coordinate of the solution to $g$. The reason for this is that the derivative of function $x^4$ is very small when $x$ is close to 0 (if we have $x_1^2$ instead of $x_1^4$ as the second term of $g(x)$, then the results are much better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a30IrBOjLXOZ"
      },
      "source": [
        "c = random.gauss(0,1)\r\n",
        "\r\n",
        "def f(x):\r\n",
        "    return np.sin(x[0])/x[0] + (x[1]-c)**2\r\n",
        "\r\n",
        "def g(x):\r\n",
        "    return 2+x[0]**4+(1+x[1])*(1+x[1])\r\n",
        "\r\n",
        "def f_fail(x):\r\n",
        "    return [x[0],x[1]]\r\n",
        "\r\n",
        "\r\n",
        "exactf = [4.4934094579090641753, c]\r\n",
        "sol_f1 = gradient_descent(f, [5,1])\r\n",
        "sol_f2 = newton(f,[5,1])\r\n",
        "np.testing.assert_array_almost_equal(exactf, sol_f1, decimal=6)\r\n",
        "np.testing.assert_array_almost_equal(exactf, sol_f2, decimal=6)\r\n",
        "# print(exactf)\r\n",
        "# print(sol_f1)\r\n",
        "# print(sol_f2)\r\n",
        "\r\n",
        "exactg = [0, -1]\r\n",
        "sol_g1 = gradient_descent(g, [1,1])\r\n",
        "sol_g2 = newton(g,[1,1])\r\n",
        "np.testing.assert_array_almost_equal(exactg, sol_g1, decimal=2)\r\n",
        "np.testing.assert_array_almost_equal(exactg, sol_g2, decimal=2)\r\n",
        "# print(exactg)\r\n",
        "# print(sol_g1)\r\n",
        "# print(sol_g2)\r\n",
        "\r\n",
        "# Invalid input test: wrong function\r\n",
        "np.testing.assert_raises(Exception, gradient_descent, f_fail, [1,1])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The methods, implemented for solving the tasks, actually succeeded in solving them, as it is confirmed by the test results.\n",
        "\n",
        "It is possible to test the methods for arbitrary functions (that return a scalar value), even $\\mathbb{R}\\to\\mathbb{R}$ functions, but in this case the initial value shall be given as an array of length 1.\n",
        "\n",
        "The initial guess $x_0$ plays an important role. First, if a wrong value is chosen, then the algorithms may find only a local minimum or not find a minimum at all. Second, it tells the algorithms the dimension of the input space: it is the number of elements in $x_0$.\n",
        "\n",
        "The precisions of both methods depend on the function that is to be minimized. For example, when the derivative is very small, it may be difficult to use the gradient for finding the minimum exactly, as computations with tiny numbers are likely to be inaccurate. This is the reason why the default value for the small constant $h$ in method \"hessian\" was chosen to be relatively large: when it was smaller, the Hessian of $g$ got even less accurate.\n",
        "\n",
        "One could use an adaptive method when calculating the best possible value for $\\alpha$: first using larger steps, and then refining those intervals, which are likely to contain the best value."
      ]
    }
  ]
}