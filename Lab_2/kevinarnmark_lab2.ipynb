{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kevinarnmark_lab2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Matrix Factorization**\n",
        "**Kevin Arnmark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "In this lab I implement algorithms for solving sparse matrix-vector product, QR factorization, solving Ax=b and blocked matrix-matrix product. The base algorithms and theory comes from the lecture notes. The accuracy of the algorithms are also tested in various ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca76782a-e861-4551-e5bc-f6b523142c7b"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common \n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "    \n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "The following functions are implemented in this report:\n",
        "\n",
        "1. **Function:** sparse matrix-vector product\n",
        "\n",
        "  **Input:** vector $x$, sparse (real, quadratic) matrix $A$: CRS arrays val, col_idx, row_ptr\n",
        "  \n",
        "  **Output:** matrix-vector product $b=Ax$\n",
        "  \n",
        "  **Test:** verify accuracy against dense matrix-vector product. \n",
        "\n",
        "2. **Function:** QR factorization\n",
        "\n",
        "  **Input:** (real quadratic) matrix $A$\n",
        "  \n",
        "  **Output:** orthogonal matrix $Q$, upper triangular matrix $R$, such that $A=QR$\n",
        "  \n",
        "  **Test:** $R$ upper triangular, Frobenius norms $|| Q^TQ-I ||_F, || QR-A ||_F$\n",
        "\n",
        "3. **Function:** direct solver $Ax=b$\n",
        "\n",
        "  **Input:** (real, quadratic) matrix $A$, vector $b$\n",
        "  **Output:** vector x=A^-1b\n",
        "  **Test:** residual || Ax-b ||, and || x-y || where y is a manufactured solution with b=Ay\n",
        "\n",
        "**Extra Task:**\n",
        "\n",
        "4. **Function:** blocked matrix-matrix product\n",
        "\n",
        "  **Input:** (real compatible) matrices $A, B$\n",
        "  \n",
        "  **Output:** matrix-matrix product $C=AB$\n",
        "  \n",
        "  **Test:** formulate test cases to verify accuracy. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxspIhFa47r6"
      },
      "source": [
        "**Sparse matrix-vector product**\r\n",
        "\r\n",
        "The compressed row storage format (CRS) takes the form of three arrays. One array containing the non zero values, another containing those values column indices, and the last pointers to the start of each row. I created a new class for this to keep the algorithm structured and easy to follow. (Chapter 5.8)\r\n",
        "\r\n",
        "The algorithm used below is from Chapter 5.8 Algorithm 5.9. The changes I made compared to the lecture notes are that I use 0 indexing instead of 1 indexing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CJXMCT1HTHL"
      },
      "source": [
        "class CRS:\r\n",
        "  #val, col_idx, row_ptr = [],[],[]\r\n",
        "  def __init__(self, val, col_idx, row_ptr):\r\n",
        "    assert len(val) == row_ptr[-1]\r\n",
        "    self.val = val\r\n",
        "    self.col_idx = col_idx\r\n",
        "    self.row_ptr = row_ptr\r\n",
        "\r\n",
        "\r\n",
        "def crs_matrix_vector_product(x, A):\r\n",
        "  b = np.zeros((len(x), ))\r\n",
        "  for i in range(len(x)):\r\n",
        "    for j in range(A.row_ptr[i], A.row_ptr[i + 1]):\r\n",
        "      b[i] += A.val[j] * x[A.col_idx[j]]\r\n",
        "  return b"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-KM06YLOoqL"
      },
      "source": [
        "**QR Factorization**\r\n",
        "\r\n",
        "QR factorization is a way to factorize a matrix $A$ by creating an orthogonal matrix $Q$ and an upper triangular matrix $R$ where $QR=A$. (Chapter 5.3)\r\n",
        "\r\n",
        "I am using the modified gram-schmidt factorization Algorithm 5.3 from Chapter 5.3. It is also possible to use the householder factorization for solving this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CHob-kOriK"
      },
      "source": [
        "def qr_factorization(A):\r\n",
        "  n = len(A)\r\n",
        "  R, Q = np.zeros((n, n)), np.zeros((n, n))\r\n",
        "  for j in range(len(A)):\r\n",
        "    v = np.array(A[:,j])\r\n",
        "    for i in range(j):\r\n",
        "      R[i,j] = np.dot(Q[:,i], v)\r\n",
        "      v = v - R[i,j]*Q[:,i]\r\n",
        "    R[j, j] = np.linalg.norm(v)\r\n",
        "    Q[:,j] = v[:] / R[j,j]\r\n",
        "\r\n",
        "  return Q, R"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rqX_v4dKXNA"
      },
      "source": [
        "**Direct Solver Ax=b**\r\n",
        "\r\n",
        "Solving $Ax=B$ is done by inverting matrix $A$ resulting in $x=A^{-1}b$. By factorizing $A$ into an orthogonal matrix $Q$ and upper triangular matrix $R$ it is easier to get the inverse of $A$. $x=A^{-1}b \\iff x=(QR)^{-1}b$.\r\n",
        "\r\n",
        "The inverse of $Q$ is the transpose of $Q$ since it is an orthogonal matrix. $Q^{-1} = Q^T$ \r\n",
        "\r\n",
        "The inverse of an upper triangular matrix can be calculated using backward substitution. The algorithm below implements an algorithm for solving backward substitution from Chapter 5.2 Algorithm 5.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKB5ecjIKmmX"
      },
      "source": [
        "def backward_substitution(U, b):\r\n",
        "  n = len(b)\r\n",
        "  x = np.zeros((n, ))\r\n",
        "  x[n - 1] = b[n - 1] / U[n-1, n-1]\r\n",
        "  for i in range(n - 2, -1, -1):\r\n",
        "    s = 0\r\n",
        "    for j in range(i + 1, n):\r\n",
        "      s += U[i, j] * x[j]\r\n",
        "    x[i] = (b[i] - s) / U[i, i]\r\n",
        "  return x\r\n",
        "\r\n",
        "def direct_solver(A, b):\r\n",
        "  assert len(A[0]) == len(b)\r\n",
        "  Q, R = qr_factorization(A)\r\n",
        "  Q_i = np.transpose(Q)\r\n",
        "  return backward_substitution(R, Q_i.dot(b))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlKNubt9ei2A"
      },
      "source": [
        "**Extra task Blocked matrix-matrix product**\r\n",
        "\r\n",
        "By dividing up each matrix into smaller blocks it is possible to reduce the number of memory references needed to calculate a matrix-matrix product. \r\n",
        "\r\n",
        "The number of memory references needed with a blocked matrix-matrix product is $m=n^2(2N+1)$ instead of $m=2n^2+n^3$, which is the case for Algorithm 5.7, where $N$ is the number of blocks and $n$ the dimension of each matrix. (Chapter 5.7)\r\n",
        "\r\n",
        "The algorithm is based on the algorithm from Chapter 5.7 Algorithm 5.8. In this algorithm i am creating as large blocks as possible since I am not going to use any matrices that divided are larger than the cache. By using as large blocks as possible the computational intensity is maximized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUwtgrSdey-D"
      },
      "source": [
        "def blocked_m_m_product(A, B):\r\n",
        "  assert len(A[0]) == len(B)\r\n",
        "  M, N, P = 2, 2, 2\r\n",
        "  m, n, p = len(A), len(B[0]), len(B)\r\n",
        "  bm, bn, bp = m/M, n/N, p/P\r\n",
        "  while (not bm.is_integer()): # Choosing as large blocks as possible\r\n",
        "    bm = m/M\r\n",
        "    M += 1\r\n",
        "  while (not bn.is_integer()):\r\n",
        "    bn = n/N\r\n",
        "    N += 1\r\n",
        "  while (not bp.is_integer()):\r\n",
        "    bp = p/P\r\n",
        "    P += 1\r\n",
        "  bm = int(bm)\r\n",
        "  bn = int(bn)\r\n",
        "  bp = int(bp)\r\n",
        "  C = np.zeros((m, n))\r\n",
        "  for i in range(M):\r\n",
        "    for j in range(N):\r\n",
        "      for k in range(P):\r\n",
        "        C[i*bm:(i+1)*bm, j*bn:(j+1)*bn] += np.matmul(A[i*bm:(i+1)*bm, k*bp:(k+1)*bp], B[k*bp:(k+1)*bp, j*bn:(j+1)*bn])\r\n",
        "\r\n",
        "  return C"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt4MijQxI7P3"
      },
      "source": [
        "# Tests:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4rBEfYmVBF-"
      },
      "source": [
        "Testing the accuracy of the algorithms. This could be done several different ways. I did a smaller amount of predetermined tests, but ultimately I would have wanted to implement some random generation of matrices and vectors to be able to run more tests to get a higher significance of the tests. But I unfortunately did not have the time for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXJtLv0VJFzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13491ae1-b195-4ab4-c5a4-ad9b575d00ad"
      },
      "source": [
        "def frobenius_norm(A):\r\n",
        "  s = 0\r\n",
        "  for i in range(len(A)):\r\n",
        "    for j in range(len(A[0])):\r\n",
        "      s += abs(A[i,j]**2)\r\n",
        "  return np.sqrt(s)\r\n",
        "\r\n",
        "# Testing sparse matrix-vector product:\r\n",
        "\r\n",
        "A = np.array([[0, 0, -2],[0, 1, 0],[9, 0, 0]])\r\n",
        "v = np.array([0, 3, 2])\r\n",
        "A_crs = CRS(np.array([-2, 1, 9]), np.array([2, 1, 0]), np.array([0, 1, 2, 3]))\r\n",
        "\r\n",
        "\r\n",
        "# Test: verify accuracy against dense matrix-vector product. \r\n",
        "assert (crs_matrix_vector_product(v, A_crs) - np.dot(A, v)).all() == 0\r\n",
        "print(\"sparse matrix-vector product tests passed\")\r\n",
        "\r\n",
        "# Testing qr_factorization(A)\r\n",
        "\r\n",
        "A = np.array([[1, 3, 2],[2, 1, 3],[3, 2, 1]])\r\n",
        "Q, R = qr_factorization(A)\r\n",
        "\r\n",
        "\r\n",
        "#Test: R upper triangular\r\n",
        "assert np.allclose(R, np.triu(R)) == True\r\n",
        "#Test: Frobenius norm || Q^TQ-I ||_F\r\n",
        "np.testing.assert_almost_equal(frobenius_norm(np.matmul(np.transpose(Q), Q) - np.identity(len(Q))), 0)\r\n",
        "#Test: Frobenius norm || QR-A ||_F\r\n",
        "np.testing.assert_almost_equal(frobenius_norm(np.matmul(Q, R) - A), 0)\r\n",
        "\r\n",
        "print(\"qr factorization tests passed\")\r\n",
        "\r\n",
        "# Testing direct solver Ax = b\r\n",
        "b = np.array([1, 2, 3])\r\n",
        "x = direct_solver(A, b)\r\n",
        "# Test: residual || Ax-b ||\r\n",
        "np.testing.assert_almost_equal(np.linalg.norm(np.dot(A,x) - b), 0)\r\n",
        "\r\n",
        "y = np.array([5, -3, 1])\r\n",
        "b = np.dot(A, y)\r\n",
        "x = direct_solver(A, b)\r\n",
        "# Test: || x-y || where y is a manufactured solution with b=Ay\r\n",
        "np.testing.assert_almost_equal(np.linalg.norm(x - y), 0)\r\n",
        "\r\n",
        "print(\"direct solver Ax = b tests passed\")\r\n",
        "\r\n",
        "# Testing the blocked matrix-matrix product\r\n",
        "\r\n",
        "A = np.array([[1, 2, 3, 4],[4, 3, 2, 1],[1, 3, 2, 4],[4, 2, 3, 1]])\r\n",
        "B = np.array([[1, -2, 2, 4],[3, 3, 4, 0],[1, 2, 1, -4],[-4, 1, 2, 0]])\r\n",
        "\r\n",
        "# Test: testing against numpy matmul to verify accuracy\r\n",
        "assert (np.matmul(A, B) - blocked_m_m_product(A, B)).all() == 0\r\n",
        "\r\n",
        "A = np.array([[1, 2, 3],[4, 3, 2],[1, 3, 2],[4, 2, 3]])\r\n",
        "B = np.array([[1, -2], [3, 3], [1, 2]])\r\n",
        "# Test: testing against numpy matmul to verify accuracy.\r\n",
        "assert (np.matmul(A, B) - blocked_m_m_product(A, B)).all() == 0\r\n",
        "print(\"blocked matrix-matrix product tests passed\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sparse matrix-vector product tests passed\n",
            "qr factorization tests passed\n",
            "direct solver Ax = b tests passed\n",
            "blocked matrix-matrix product tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WhajvnO3y1K"
      },
      "source": [
        "The algorithms passes all the tests, meaning that they are of sufficient accuracy. They do however have limited precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "If I had more time I would have wanted to write some test to verify that the blocked matrix-matrix product uses less memory references. It is currently only testing the accuracy of the product."
      ]
    }
  ]
}