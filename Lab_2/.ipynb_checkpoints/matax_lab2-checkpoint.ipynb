{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT20/blob/master/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RgtXlfYO_i7"
   },
   "source": [
    "# **Lab 2: Matrix Factorization**\n",
    "**Mathias Axelsson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x_J5FVuPzbm"
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJipbXtnjrJZ"
   },
   "source": [
    "This report will implement functions for sparse matrix-vector products, a direct matrix equation solver, a QR matrix factorization and a function for blocked matrix products. These functions will then be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkT8J7uOWpT3"
   },
   "source": [
    "#**About the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmB2noTr1Oyo"
   },
   "source": [
    "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pdll1Xc9WP0e",
    "outputId": "6435158d-714b-4fa0-f688-bbd72fa2bc02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
    "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
    "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
    "\n",
    "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This template is maintained by Johan Hoffman\n",
    "# Please report problems to jhoffman@kth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28xLGz8JX3Hh"
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2PYNusD08Wa"
   },
   "source": [
    "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw7VlErAX7NS"
   },
   "outputs": [],
   "source": [
    "# Load neccessary modules.\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import tri\n",
    "from matplotlib import axes\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnO3lhAigLev"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5zMzgPlRAF6"
   },
   "source": [
    "In this report functions for sparse matrix-vector products, a direct matrix equation solver and a QR matrix factorization. The sparse matrix-vector product utilizes the fact that a matrix with most of its elements set to zero can be represented as three vectors significantly lowering the computational complexity of the function.\n",
    "\n",
    "The QR matrix factorization function finds an orthonogal matrix $Q$ and a upper triangular matrix $R$ such that $QR = A$. This is usefull for finding the inverse of a function as orthogonal and triangular matrices are easy to invert.\n",
    "\n",
    "The direct matrix equation solver takes in a $A$ and $b$ and finds a $x$ such that $Ax = b$. It does this by first using QR matrix factorization and then multiplying $Ax = b$ with $Q^T$ resulting in $Rx = Q^Tb$. This is then solved using backwards substitution. \n",
    "\n",
    "The blocked matrix product divides each matrix into smaller blocks. By doing this less memory references are needed per function call. This increases the computational intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOQvukXZq5U5"
   },
   "source": [
    "# **Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Sparse matrix-vector product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mv_product(x, val, col_idx, row_ptr):\n",
    "    b = np.zeros(x.shape) # Since A is square.\n",
    "    for i in range(len(x)):\n",
    "        for j in range(row_ptr[i], row_ptr[i+1]):\n",
    "            b[i] += val[j-1]*x[col_idx[j-1]-1]\n",
    "    return b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### QR factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR_factorization(A): # Gram-Schmidt\n",
    "    Q = np.zeros(A.shape)\n",
    "    R = np.zeros(A.shape)\n",
    "    \n",
    "    if A.size[0] != A.size[1]:\n",
    "        print(\"A must be square\")\n",
    "        raise Exception\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        v = A[:,i]\n",
    "        for j in range(i):\n",
    "            R[j,i] = np.dot(Q[:,j],v)\n",
    "            v = v - R[j,i]*Q[:,j]\n",
    "        R[i,i] = np.linalg.norm(v)\n",
    "        Q[:,i] = v/R[i,i]\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Direct solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_solver(A, b):\n",
    "    Q, R = QR_factorization(A)\n",
    "    Qb = np.matmul(Q.transpose(), b)\n",
    "    \n",
    "    # Solve Rx = Q^-1 b\n",
    "    \n",
    "    if A.size[0] != b.size:\n",
    "        print(\"A and b must have compatible dimmensions.\")\n",
    "        raise Exception\n",
    "    \n",
    "    x = np.zeros(b.shape) # A quadratic\n",
    "    n = len(x)\n",
    "    \n",
    "    x[n-1] = Qb[n-1]/R[n-1,n-1]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        s = 0\n",
    "        for j in range(i+1, n):  \n",
    "            s += R[i,j]*x[j]\n",
    "        x[i] = (Qb[i] - s)/R[i,i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF4iBj5VURZx"
   },
   "source": [
    "#### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocked_mm_product(A, B, blockSize=(3, 3, 3)):\n",
    "    (m, n1) = A.shape\n",
    "    (n2, p) = B.shape\n",
    "    \n",
    "    if n1 != n2:\n",
    "        print(\"Matrices must be multipliable.\")\n",
    "        raise Exception\n",
    "        \n",
    "    n = n1\n",
    "    \n",
    "    # Sizes of blocks\n",
    "    sm = blockSize[0]\n",
    "    sn = blockSize[1]\n",
    "    sp = blockSize[2]\n",
    "    \n",
    "    # Number of blocks with blockSize\n",
    "    bm = np.ceil(m/sm)\n",
    "    bn = np.ceil(n/sn)\n",
    "    bp = np.ceil(p/sp)\n",
    "    \n",
    "    C = np.zeros((m, p))\n",
    "    \n",
    "    \n",
    "    for i in range(bm.astype(int)):\n",
    "        for j in range(bn.astype(int)):\n",
    "            for k in range(bp.astype(int)):\n",
    "                # Blocks that overlap outside the matrices are reduced to the remainder of the matrices.\n",
    "                # Therefore no case where n/sm being a fraction need to be considered.\n",
    "                blockProduct = A[i*sm:(i+1)*sm, k*sp:(k+1)*sp] @ B[k*sp:(k+1)*sp, j*sn:(j+1)*sn] \n",
    "                C[i*sm:(i+1)*sm, j*sn:(j+1)*sn] = C[i*sm:(i+1)*sm, j*sn:(j+1)*sn] + blockProduct\n",
    "                \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsQLT38gVbn_"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLwlnOzuV-Cd"
   },
   "source": [
    "Present the results. If the result is an algorithm that you have described under the *Methods* section, you can present the data from verification and performance tests in this section. If the result is the output from a computational experiment this is where you present a selection of that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, -1, -1, 1, -1, 1])\n",
    "val = np.array([3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 3])\n",
    "col_idx = np.array([1, 2, 4, 2, 3, 3, 3, 4, 5, 5, 6])\n",
    "row_ptr = np.array([1, 4, 6, 7, 9, 10, 12])\n",
    "\n",
    "assert np.array([3, -3, -1, -1, -1, 1]).all() == sparse_mv_product(x, val, col_idx, row_ptr).all()\n",
    "assert np.array([0, 0, 0, 0, 0, 0]).all() == sparse_mv_product(np.array([0, 0, 0, 0, 0, 0]), val, col_idx, row_ptr).all()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    A = np.random.randint(20, size=(10,10))\n",
    "    Q, R = QR_factorization(A)\n",
    "    \n",
    "    try:\n",
    "        assert np.linalg.norm(Q.transpose() @ Q - np.identity(Q.shape[0]), ord=\"fro\") < 10e-10 # Calculation are not exact\n",
    "        assert np.linalg.norm(Q @ R - A, ord=\"fro\") < 10e-10\n",
    "    except:\n",
    "        print(\"Error with matrix:\")\n",
    "        print(A)\n",
    "\n",
    "for i in range(1):\n",
    "    A = np.random.randint(20, size=(10,10))\n",
    "       \n",
    "    b1 = np.random.randint(20, size=10)\n",
    "    x1 = direct_solver(A, b1)\n",
    "    y = np.random.randint(20, size=10)\n",
    "    b2 = np.matmul(A, y)\n",
    "    x2 = direct_solver(A, b2)\n",
    "    try:\n",
    "        assert np.linalg.norm(np.matmul(A, x1) - b1) < 10e-10\n",
    "        assert np.linalg.norm(x2 - y) < 10e-10\n",
    "    except:\n",
    "        print(\"Error with matrices:\")\n",
    "        print(A)\n",
    "        print(b1, x1)\n",
    "        print(y, b2)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    A = np.random.randint(20, size=(10,10))\n",
    "    B = np.random.randint(20, size=(10,10))\n",
    "    try:\n",
    "        assert blocked_mm_product(A, B, blockSize=(3, 3, 3)).all() == np.matmul(A, B).all()\n",
    "    except:\n",
    "        print(\"Error with matrices:\")\n",
    "        print(A)\n",
    "        print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4GLBv0zWr7m"
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bcsDSoRXHZe"
   },
   "source": [
    "The functions work as expected. One thing that I noticed was that the rounding errors increased with larger matrices. This is of course expected as a larger matrix will have more calculations. The errors are still small when compared to the values of the matrices. There would be a risk of losing information if some elements were small but nonzero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "template-report-lab-X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
